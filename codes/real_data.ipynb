{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(X,Z,D,Y,alpha,beta,eta,gamma,estimator='LATE'):\n",
    "    # print(X.type(), beta.type())\n",
    "    phi = torch.sigmoid(X@beta)\n",
    "    c1, c3 = phi[:,0], phi[:,1]\n",
    "    if estimator == 'MLATE':\n",
    "        c0 = torch.exp(X @ alpha)\n",
    "    c5 = torch.exp(X@eta)\n",
    "    # c5 = c5 * (torch.abs(c5-1) > 1e-10) + (torch.abs(c5-1) <= 1e-10) * (c5-1 >= 0.0) * (1+1e-10) + (torch.abs(c5-1) <= 1e-10) * (c5-1< 0.0) * (1-1e-10)\n",
    "    # c5 = 1 + c5 - c5.clamp(1-1e-5,1+1e-5)\n",
    "    c0 = c0.clamp(1e-10)\n",
    "    if estimator == 'MLATE':\n",
    "        f0 = torch.where(torch.abs(c5-1)>1e-10, (-(c0+1)*c5+torch.sqrt(c5**2*(c0-1)**2 + 4*c0*c5)) / (2*c0*(1-c5)), -(-c0-1+(2*(c0-1)**2+4*c0)/(c0+1)/2)/(2*c0))\n",
    "        f1 = f0 * c0\n",
    "    p011 = (1-c1)*c3\n",
    "    p001 = (1-c1)*(1-c3)\n",
    "    p110 = 0\n",
    "    p100 = 0\n",
    "    p111 = f1*c1 + p110\n",
    "    p010 = f0*c1 + p011\n",
    "    p101 = 1-p001-p011-p111\n",
    "    p000 = 1-p010-p100-p110\n",
    "\n",
    "    d = torch.sigmoid(X@gamma)\n",
    "    l = D*Y*Z*p111*d + (1-D)*Y*Z*p011*d + D*(1-Y)*Z*p101*d + (1-D)*(1-Y)*Z*p001*d + (1-D)*Y*(1-Z)*p010*(1-d) + (1-D)*(1-Y)*(1-Z)*p000*(1-d)\n",
    "    # if torch.mean(torch.log(l.clamp(1e-10, 1))) :\n",
    "    #     print(l)\n",
    "    #     sys.exit(0)\n",
    "    return torch.mean(torch.log(l.clamp(1e-10)))\n",
    "\n",
    "def square_loss(X, Z, D, Y, alpha, beta, gamma, eta, estimator, strategy='identity'):\n",
    "    d = torch.sigmoid(X@gamma)\n",
    "    phi = torch.sigmoid(X@beta)\n",
    "    phi1, phi3 = phi[:,0], phi[:,1]\n",
    "    OP = torch.exp(X@eta)\n",
    "    f = (d**Z) * ((1-d)**(1-Z))\n",
    "    if estimator == 'MLATE':\n",
    "        theta = torch.exp(X@alpha)\n",
    "        H = Y * theta**(-D)\n",
    "        f0 = torch.where(torch.abs(OP-1)>1e-10, (-(theta+1)*OP+torch.sqrt(OP**2*(theta-1)**2+4*theta*OP)) / (2*theta*(1-OP)), -(-theta-1+(2*(theta-1)**2+4*theta)/(theta+1)/2)/(2*theta))\n",
    "        f1 = f0 * theta\n",
    "        E = f0*phi1 +(1-phi1)*phi3\n",
    "    \n",
    "    if strategy == 'identity':\n",
    "        return torch.sum((torch.sum(X*((2*Z-1)*(H-E)/f).unsqueeze(1), dim=0))**2)\n",
    "    elif strategy == 'optimal':\n",
    "        p011 = (1-phi1)*phi3\n",
    "        p001 = (1-phi1)*(1-phi3)\n",
    "        p110 = 0\n",
    "        p100 = 0\n",
    "        p111 = f1*phi1 + p110\n",
    "        p010 = f0*phi1 + p011\n",
    "        p101 = 1-p001-p011-p111\n",
    "        p000 = 1-p010-p100-p110\n",
    "        if estimator == 'MLATE':\n",
    "            EH2_1 = p111/theta**2+p101\n",
    "            EH2_0 = p110/theta**2+p100\n",
    "            EH_1 = p111/theta+p101\n",
    "            EH_0 = p110/theta+p100\n",
    "            EZX = (EH2_1-EH_1**2) / d + (EH2_0-EH_0**2)/(1-d)\n",
    "            w = -X * (1 / theta * f1 * phi1 / EZX).unsqueeze(1)\n",
    "            return torch.mean((torch.mean(w*((2*Z-1)*(H-E)/f).unsqueeze(1), dim=0))**2)\n",
    "\n",
    "def MLE(X, Z, D ,Y, estimator='MLATE', dr=True):\n",
    "    N, p = X.shape\n",
    "    alpha = nn.Parameter(torch.rand(size=(p,))*0.2-0.1)\n",
    "    beta = nn.Parameter(torch.rand(size=(p,2))*0.2-0.1) ## only phi1 and phi3\n",
    "    eta = nn.Parameter(torch.rand(size=(p,))*2-1)\n",
    "    gamma = nn.Parameter(torch.rand(size=(p,))*0.2-0.1)\n",
    "    opt = torch.optim.Adam(params=(alpha, beta, eta, gamma), lr=1e-3, weight_decay=0)\n",
    "    optloss = float('inf')\n",
    "    for i in range(50000):\n",
    "        opt.zero_grad()\n",
    "        loss = -nll(X,Z,D,Y,alpha,beta,eta,gamma, estimator)\n",
    "        if loss.item() < optloss:\n",
    "            if abs(loss.item() - optloss) < 1e-6:\n",
    "                break\n",
    "            optloss = loss.item()\n",
    "            mlealpha = alpha.detach().clone()\n",
    "            mlebeta = beta.detach().clone()\n",
    "            mleeta = eta.detach().clone()\n",
    "            mlegamma = gamma.detach().clone()\n",
    "        if i % 100 ==0: \n",
    "            print('Iter {} | loss {:.04f}'.format(i+1, loss.item()))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if torch.sum(loss.isnan()) > 0:\n",
    "            break\n",
    "    if not dr:\n",
    "        return mlealpha, mlebeta, mleeta, mlegamma\n",
    "        \n",
    "    alpha = nn.Parameter(mlealpha.clone(),requires_grad=True)\n",
    "    # alpha = nn.Parameter(torch.rand(size=(2,))*2-1)\n",
    "    opt = torch.optim.Adam(params=(alpha,), lr=1e-3, weight_decay=0)\n",
    "    sqoptloss = float('inf')\n",
    "    for i in range(20000):\n",
    "        opt.zero_grad()\n",
    "        sq_loss = square_loss(X, Z, D, Y, alpha, mlebeta, mlegamma, mleeta, estimator, strategy='optimal')\n",
    "        if i % 100 ==0:\n",
    "            print('Iter {} | sq_loss {:.08f}'.format(i+1, sq_loss.item()))\n",
    "        if sq_loss.item() < sqoptloss:\n",
    "            sqoptloss = sq_loss.item()\n",
    "            drwalpha = alpha.detach().clone()\n",
    "            if abs(sqoptloss) < 1e-6:\n",
    "                break\n",
    "        sq_loss.backward()\n",
    "        opt.step()\n",
    "        if torch.sum(sq_loss.isnan()) > 0:\n",
    "            break\n",
    "    return mlealpha, drwalpha, mlebeta, mleeta, mlegamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 1 --------------------------------------------------\n",
      "Iter 1 | loss 1.5797\n",
      "Iter 101 | loss 1.5060\n",
      "Iter 201 | loss 1.4943\n",
      "Iter 301 | loss 1.4887\n",
      "Iter 401 | loss 1.4853\n",
      "Iter 501 | loss 1.4837\n",
      "Iter 601 | loss 1.4807\n",
      "Iter 701 | loss 1.4795\n",
      "Iter 801 | loss 1.4786\n",
      "Iter 901 | loss 1.4780\n",
      "Iter 1001 | loss 1.4775\n",
      "Iter 1101 | loss 1.4770\n",
      "Iter 1201 | loss 1.4767\n",
      "Iter 1301 | loss 1.4763\n",
      "Iter 1401 | loss 1.4760\n",
      "Iter 1501 | loss 1.4758\n",
      "Iter 1601 | loss 1.4755\n",
      "Iter 1701 | loss 1.4753\n",
      "Iter 1801 | loss 1.4750\n",
      "Iter 1901 | loss 1.4748\n",
      "Iter 2001 | loss 1.4746\n",
      "Iter 2101 | loss 1.4743\n",
      "Iter 2201 | loss 1.4741\n",
      "Iter 2301 | loss 1.4739\n",
      "Iter 2401 | loss 1.4736\n",
      "Iter 2501 | loss 1.4734\n",
      "Iter 2601 | loss 1.4732\n",
      "Iter 2701 | loss 1.4729\n",
      "Iter 2801 | loss 1.4727\n",
      "Iter 2901 | loss 1.4725\n",
      "Iter 3001 | loss 1.4722\n",
      "Iter 3101 | loss 1.4720\n",
      "Iter 3201 | loss 1.4718\n",
      "Iter 3301 | loss 1.4715\n",
      "Iter 3401 | loss 1.4713\n",
      "Iter 3501 | loss 1.4711\n",
      "Iter 3601 | loss 1.4709\n",
      "Iter 3701 | loss 1.4706\n",
      "Iter 3801 | loss 1.4704\n",
      "Iter 3901 | loss 1.4702\n",
      "Iter 4001 | loss 1.4700\n",
      "Iter 4101 | loss 1.4698\n",
      "Iter 4201 | loss 1.4695\n",
      "Iter 4301 | loss 1.4693\n",
      "Iter 4401 | loss 1.4691\n",
      "Iter 4501 | loss 1.4689\n",
      "Iter 4601 | loss 1.4687\n",
      "Iter 4701 | loss 1.4685\n",
      "Iter 4801 | loss 1.4683\n",
      "Iter 4901 | loss 1.4681\n",
      "Iter 5001 | loss 1.4679\n",
      "Iter 5101 | loss 1.4678\n",
      "Iter 5201 | loss 1.4676\n",
      "Iter 5301 | loss 1.4674\n",
      "Iter 5401 | loss 1.4672\n",
      "Iter 5501 | loss 1.4670\n",
      "Iter 5601 | loss 1.4669\n",
      "Iter 5701 | loss 1.4667\n",
      "Iter 5801 | loss 1.4665\n",
      "Iter 5901 | loss 1.4664\n",
      "Iter 6001 | loss 1.4662\n",
      "Iter 6101 | loss 1.4661\n",
      "Iter 6201 | loss 1.4659\n",
      "Iter 6301 | loss 1.4658\n",
      "Iter 6401 | loss 1.4656\n",
      "Iter 1 | sq_loss 0.00000652\n",
      "Bootstrap 2 --------------------------------------------------\n",
      "Iter 1 | loss 2.1402\n",
      "Iter 101 | loss 1.7326\n",
      "Iter 201 | loss 1.6661\n",
      "Iter 301 | loss 1.6426\n",
      "Iter 401 | loss 1.6318\n",
      "Iter 501 | loss 1.6253\n",
      "Iter 601 | loss 1.6194\n",
      "Iter 701 | loss 1.6097\n",
      "Iter 801 | loss 1.5794\n",
      "Iter 901 | loss 1.5316\n",
      "Iter 1001 | loss 1.5000\n",
      "Iter 1101 | loss 1.4898\n",
      "Iter 1 | sq_loss 0.00378685\n",
      "Iter 101 | sq_loss 0.00001637\n",
      "Iter 201 | sq_loss 0.00001576\n",
      "Iter 301 | sq_loss 0.00001500\n",
      "Iter 401 | sq_loss 0.00001412\n",
      "Iter 501 | sq_loss 0.00001315\n",
      "Iter 601 | sq_loss 0.00001211\n",
      "Iter 701 | sq_loss 0.00001108\n",
      "Iter 801 | sq_loss 0.00001002\n",
      "Iter 901 | sq_loss 0.00000899\n",
      "Iter 1001 | sq_loss 0.00000800\n",
      "Iter 1101 | sq_loss 0.00000706\n",
      "Iter 1201 | sq_loss 0.00000618\n",
      "Iter 1301 | sq_loss 0.00000539\n",
      "Iter 1401 | sq_loss 0.00000467\n",
      "Iter 1501 | sq_loss 0.00000404\n",
      "Iter 1601 | sq_loss 0.00000351\n",
      "Iter 1701 | sq_loss 0.00000305\n",
      "Iter 1801 | sq_loss 0.00000266\n",
      "Iter 1901 | sq_loss 0.00000236\n",
      "Iter 2001 | sq_loss 0.00000210\n",
      "Iter 2101 | sq_loss 0.00000190\n",
      "Iter 2201 | sq_loss 0.00000175\n",
      "Iter 2301 | sq_loss 0.00000163\n",
      "Iter 2401 | sq_loss 0.00000153\n",
      "Iter 2501 | sq_loss 0.00000146\n",
      "Iter 2601 | sq_loss 0.00000140\n",
      "Iter 2701 | sq_loss 0.00000135\n",
      "Iter 2801 | sq_loss 0.00000131\n",
      "Iter 2901 | sq_loss 0.00000127\n",
      "Iter 3001 | sq_loss 0.00000124\n",
      "Iter 3101 | sq_loss 0.00000120\n",
      "Iter 3201 | sq_loss 0.00000117\n",
      "Iter 3301 | sq_loss 0.00000114\n",
      "Iter 3401 | sq_loss 0.00000111\n",
      "Iter 3501 | sq_loss 0.00000107\n",
      "Iter 3601 | sq_loss 0.00000104\n",
      "Iter 3701 | sq_loss 0.00000100\n",
      "Bootstrap 3 --------------------------------------------------\n",
      "Iter 1 | loss 1.9855\n",
      "Iter 101 | loss 1.6978\n",
      "Iter 201 | loss 1.6328\n",
      "Iter 301 | loss 1.5934\n",
      "Iter 401 | loss 1.5471\n",
      "Iter 501 | loss 1.5027\n",
      "Iter 1 | sq_loss 0.00284936\n",
      "Iter 101 | sq_loss 0.00000157\n",
      "Iter 201 | sq_loss 0.00000152\n",
      "Iter 301 | sq_loss 0.00000152\n",
      "Iter 401 | sq_loss 0.00000152\n",
      "Iter 501 | sq_loss 0.00000151\n",
      "Iter 601 | sq_loss 0.00000151\n",
      "Iter 701 | sq_loss 0.00000151\n",
      "Iter 801 | sq_loss 0.00000151\n",
      "Iter 901 | sq_loss 0.00000150\n",
      "Iter 1001 | sq_loss 0.00000150\n",
      "Iter 1101 | sq_loss 0.00000150\n",
      "Iter 1201 | sq_loss 0.00000150\n",
      "Iter 1301 | sq_loss 0.00000149\n",
      "Iter 1401 | sq_loss 0.00000149\n",
      "Iter 1501 | sq_loss 0.00000149\n",
      "Iter 1601 | sq_loss 0.00000149\n",
      "Iter 1701 | sq_loss 0.00000148\n",
      "Iter 1801 | sq_loss 0.00000148\n",
      "Iter 1901 | sq_loss 0.00000148\n",
      "Iter 2001 | sq_loss 0.00000148\n",
      "Iter 2101 | sq_loss 0.00000148\n",
      "Iter 2201 | sq_loss 0.00000147\n",
      "Iter 2301 | sq_loss 0.00000147\n",
      "Iter 2401 | sq_loss 0.00000147\n",
      "Iter 2501 | sq_loss 0.00000146\n",
      "Iter 2601 | sq_loss 0.00000146\n",
      "Iter 2701 | sq_loss 0.00000146\n",
      "Iter 2801 | sq_loss 0.00000145\n",
      "Iter 2901 | sq_loss 0.00000145\n",
      "Iter 3001 | sq_loss 0.00000145\n",
      "Iter 3101 | sq_loss 0.00000144\n",
      "Iter 3201 | sq_loss 0.00000144\n",
      "Iter 3301 | sq_loss 0.00000144\n",
      "Iter 3401 | sq_loss 0.00000143\n",
      "Iter 3501 | sq_loss 0.00000143\n",
      "Iter 3601 | sq_loss 0.00000142\n",
      "Iter 3701 | sq_loss 0.00000142\n",
      "Iter 3801 | sq_loss 0.00000141\n",
      "Iter 3901 | sq_loss 0.00000141\n",
      "Iter 4001 | sq_loss 0.00000140\n",
      "Iter 4101 | sq_loss 0.00000140\n",
      "Iter 4201 | sq_loss 0.00000139\n",
      "Iter 4301 | sq_loss 0.00000139\n",
      "Iter 4401 | sq_loss 0.00000138\n",
      "Iter 4501 | sq_loss 0.00000138\n",
      "Iter 4601 | sq_loss 0.00000137\n",
      "Iter 4701 | sq_loss 0.00000136\n",
      "Iter 4801 | sq_loss 0.00000136\n",
      "Iter 4901 | sq_loss 0.00000135\n",
      "Iter 5001 | sq_loss 0.00000134\n",
      "Iter 5101 | sq_loss 0.00000134\n",
      "Iter 5201 | sq_loss 0.00000133\n",
      "Iter 5301 | sq_loss 0.00000132\n",
      "Iter 5401 | sq_loss 0.00000131\n",
      "Iter 5501 | sq_loss 0.00000131\n",
      "Iter 5601 | sq_loss 0.00000130\n",
      "Iter 5701 | sq_loss 0.00000129\n",
      "Iter 5801 | sq_loss 0.00000128\n",
      "Iter 5901 | sq_loss 0.00000127\n",
      "Iter 6001 | sq_loss 0.00000126\n",
      "Iter 6101 | sq_loss 0.00000125\n",
      "Iter 6201 | sq_loss 0.00000124\n",
      "Iter 6301 | sq_loss 0.00000123\n",
      "Iter 6401 | sq_loss 0.00000122\n",
      "Iter 6501 | sq_loss 0.00000121\n",
      "Iter 6601 | sq_loss 0.00000119\n",
      "Iter 6701 | sq_loss 0.00000118\n",
      "Iter 6801 | sq_loss 0.00000117\n",
      "Iter 6901 | sq_loss 0.00000115\n",
      "Iter 7001 | sq_loss 0.00000114\n",
      "Iter 7101 | sq_loss 0.00000113\n",
      "Iter 7201 | sq_loss 0.00000111\n",
      "Iter 7301 | sq_loss 0.00000110\n",
      "Iter 7401 | sq_loss 0.00000108\n",
      "Iter 7501 | sq_loss 0.00000106\n",
      "Iter 7601 | sq_loss 0.00000105\n",
      "Iter 7701 | sq_loss 0.00000103\n",
      "Iter 7801 | sq_loss 0.00000101\n",
      "Bootstrap 4 --------------------------------------------------\n",
      "Iter 1 | loss 1.6683\n",
      "Iter 101 | loss 1.4952\n",
      "Iter 201 | loss 1.4749\n",
      "Iter 301 | loss 1.4719\n",
      "Iter 401 | loss 1.4708\n",
      "Iter 501 | loss 1.4701\n",
      "Iter 601 | loss 1.4696\n",
      "Iter 701 | loss 1.4691\n",
      "Iter 801 | loss 1.4687\n",
      "Iter 901 | loss 1.4683\n",
      "Iter 1001 | loss 1.4679\n",
      "Iter 1101 | loss 1.4676\n",
      "Iter 1201 | loss 1.4672\n",
      "Iter 1301 | loss 1.4669\n",
      "Iter 1401 | loss 1.4666\n",
      "Iter 1501 | loss 1.4664\n",
      "Iter 1601 | loss 1.4661\n",
      "Iter 1701 | loss 1.4659\n",
      "Iter 1801 | loss 1.4656\n",
      "Iter 1901 | loss 1.4654\n",
      "Iter 2001 | loss 1.4652\n",
      "Iter 2101 | loss 1.4650\n",
      "Iter 2201 | loss 1.4648\n",
      "Iter 2301 | loss 1.4646\n",
      "Iter 2401 | loss 1.4645\n",
      "Iter 2501 | loss 1.4643\n",
      "Iter 2601 | loss 1.4641\n",
      "Iter 2701 | loss 1.4640\n",
      "Iter 2801 | loss 1.4639\n",
      "Iter 1 | sq_loss 0.00000064\n",
      "Bootstrap 5 --------------------------------------------------\n",
      "Iter 1 | loss 1.7507\n",
      "Iter 101 | loss 1.5499\n",
      "Iter 201 | loss 1.5251\n",
      "Iter 301 | loss 1.5123\n",
      "Iter 401 | loss 1.5034\n",
      "Iter 501 | loss 1.4996\n",
      "Iter 601 | loss 1.4980\n",
      "Iter 701 | loss 1.4965\n",
      "Iter 801 | loss 1.4931\n",
      "Iter 1 | sq_loss 0.00144699\n",
      "Iter 101 | sq_loss 0.00004347\n",
      "Iter 201 | sq_loss 0.00004188\n",
      "Iter 301 | sq_loss 0.00003978\n",
      "Iter 401 | sq_loss 0.00003723\n",
      "Iter 501 | sq_loss 0.00003429\n",
      "Iter 601 | sq_loss 0.00003100\n",
      "Iter 701 | sq_loss 0.00002745\n",
      "Iter 801 | sq_loss 0.00002370\n",
      "Iter 901 | sq_loss 0.00001988\n",
      "Iter 1001 | sq_loss 0.00001613\n",
      "Iter 1101 | sq_loss 0.00001259\n",
      "Iter 1201 | sq_loss 0.00000943\n",
      "Iter 1301 | sq_loss 0.00000675\n",
      "Iter 1401 | sq_loss 0.00000464\n",
      "Iter 1501 | sq_loss 0.00000309\n",
      "Iter 1601 | sq_loss 0.00000205\n",
      "Iter 1701 | sq_loss 0.00000139\n",
      "Iter 1801 | sq_loss 0.00000101\n",
      "Bootstrap 6 --------------------------------------------------\n",
      "Iter 1 | loss 1.6789\n",
      "Iter 101 | loss 1.5216\n",
      "Iter 201 | loss 1.5066\n",
      "Iter 301 | loss 1.5014\n",
      "Iter 401 | loss 1.4990\n",
      "Iter 501 | loss 1.4987\n",
      "Iter 601 | loss 1.5010\n",
      "Iter 701 | loss 1.5002\n",
      "Iter 801 | loss 1.5096\n",
      "Iter 901 | loss 1.5092\n",
      "Iter 1001 | loss 1.4995\n",
      "Iter 1101 | loss 1.4991\n",
      "Iter 1201 | loss 1.4988\n",
      "Iter 1301 | loss 1.4987\n",
      "Iter 1401 | loss 1.4984\n",
      "Iter 1501 | loss 1.4982\n",
      "Iter 1601 | loss 1.4980\n",
      "Iter 1 | sq_loss 0.00379905\n",
      "Bootstrap 7 --------------------------------------------------\n",
      "Iter 1 | loss 1.7162\n",
      "Iter 101 | loss 1.5170\n",
      "Iter 201 | loss 1.4904\n",
      "Iter 301 | loss 1.4866\n",
      "Iter 401 | loss 1.4845\n",
      "Iter 501 | loss 1.4823\n",
      "Iter 601 | loss 1.4798\n",
      "Iter 701 | loss 1.4769\n",
      "Iter 801 | loss 1.4741\n",
      "Iter 901 | loss 1.4720\n",
      "Iter 1001 | loss 1.4707\n",
      "Iter 1101 | loss 1.4697\n",
      "Iter 1201 | loss 1.4690\n",
      "Iter 1301 | loss 1.4684\n",
      "Iter 1401 | loss 1.4679\n",
      "Iter 1501 | loss 1.4675\n",
      "Iter 1601 | loss 1.4671\n",
      "Iter 1701 | loss 1.4667\n",
      "Iter 1801 | loss 1.4663\n",
      "Iter 1901 | loss 1.4660\n",
      "Iter 2001 | loss 1.4657\n",
      "Iter 2101 | loss 1.4654\n",
      "Iter 2201 | loss 1.4651\n",
      "Iter 2301 | loss 1.4649\n",
      "Iter 2401 | loss 1.4646\n",
      "Iter 2501 | loss 1.4644\n",
      "Iter 2601 | loss 1.4642\n",
      "Iter 2701 | loss 1.4640\n",
      "Iter 2801 | loss 1.4638\n",
      "Iter 2901 | loss 1.4636\n",
      "Iter 3001 | loss 1.4635\n",
      "Iter 3101 | loss 1.4633\n",
      "Iter 3201 | loss 1.4632\n",
      "Iter 3301 | loss 1.4631\n",
      "Iter 1 | sq_loss 0.00000016\n",
      "Bootstrap 8 --------------------------------------------------\n",
      "Iter 1 | loss 1.6220\n",
      "Iter 101 | loss 1.4894\n",
      "Iter 201 | loss 1.4773\n",
      "Iter 301 | loss 1.4741\n",
      "Iter 401 | loss 1.4718\n",
      "Iter 501 | loss 1.4699\n",
      "Iter 601 | loss 1.4685\n",
      "Iter 701 | loss 1.4674\n",
      "Iter 801 | loss 1.4665\n",
      "Iter 901 | loss 1.4659\n",
      "Iter 1001 | loss 1.4654\n",
      "Iter 1101 | loss 1.4650\n",
      "Iter 1201 | loss 1.4647\n",
      "Iter 1301 | loss 1.4644\n",
      "Iter 1401 | loss 1.4642\n",
      "Iter 1501 | loss 1.4641\n",
      "Iter 1601 | loss 1.4639\n",
      "Iter 1 | sq_loss 0.00000009\n",
      "Bootstrap 9 --------------------------------------------------\n",
      "Iter 1 | loss 1.7001\n",
      "Iter 101 | loss 1.5482\n",
      "Iter 201 | loss 1.4999\n",
      "Iter 301 | loss 1.4846\n",
      "Iter 401 | loss 1.4820\n",
      "Iter 501 | loss 1.4807\n",
      "Iter 601 | loss 1.4795\n",
      "Iter 701 | loss 1.4780\n",
      "Iter 801 | loss 1.4763\n",
      "Iter 901 | loss 1.4740\n",
      "Iter 1001 | loss 1.4711\n",
      "Iter 1101 | loss 1.4683\n",
      "Iter 1201 | loss 1.4668\n",
      "Iter 1301 | loss 1.4663\n",
      "Iter 1401 | loss 1.4660\n",
      "Iter 1501 | loss 1.4658\n",
      "Iter 1601 | loss 1.4656\n",
      "Iter 1701 | loss 1.4654\n",
      "Iter 1801 | loss 1.4652\n",
      "Iter 1901 | loss 1.4651\n",
      "Iter 2001 | loss 1.4649\n",
      "Iter 2101 | loss 1.4647\n",
      "Iter 2201 | loss 1.4646\n",
      "Iter 2301 | loss 1.4644\n",
      "Iter 2401 | loss 1.4642\n",
      "Iter 2501 | loss 1.4641\n",
      "Iter 2601 | loss 1.4640\n",
      "Iter 1 | sq_loss 0.00000051\n",
      "Bootstrap 10 --------------------------------------------------\n",
      "Iter 1 | loss 1.7599\n",
      "Iter 101 | loss 1.6119\n",
      "Iter 201 | loss 1.5487\n",
      "Iter 301 | loss 1.5310\n",
      "Iter 401 | loss 1.5277\n",
      "Iter 501 | loss 1.5263\n",
      "Iter 601 | loss 1.5275\n",
      "Iter 701 | loss 1.5185\n",
      "Iter 801 | loss 1.5182\n",
      "Iter 901 | loss 1.5179\n",
      "Iter 1001 | loss 1.5177\n",
      "Iter 1101 | loss 1.5174\n",
      "Iter 1201 | loss 1.5172\n",
      "Iter 1 | sq_loss 0.00565478\n",
      "Iter 101 | sq_loss 0.00004011\n",
      "Iter 201 | sq_loss 0.00003804\n",
      "Iter 301 | sq_loss 0.00003559\n",
      "Iter 401 | sq_loss 0.00003280\n",
      "Iter 501 | sq_loss 0.00002979\n",
      "Iter 601 | sq_loss 0.00002669\n",
      "Iter 701 | sq_loss 0.00002357\n",
      "Iter 801 | sq_loss 0.00002053\n",
      "Iter 901 | sq_loss 0.00001763\n",
      "Iter 1001 | sq_loss 0.00001494\n",
      "Iter 1101 | sq_loss 0.00001249\n",
      "Iter 1201 | sq_loss 0.00001032\n",
      "Iter 1301 | sq_loss 0.00000843\n",
      "Iter 1401 | sq_loss 0.00000684\n",
      "Iter 1501 | sq_loss 0.00000552\n",
      "Iter 1601 | sq_loss 0.00000445\n",
      "Iter 1701 | sq_loss 0.00000360\n",
      "Iter 1801 | sq_loss 0.00000295\n",
      "Iter 1901 | sq_loss 0.00000246\n",
      "Iter 2001 | sq_loss 0.00000210\n",
      "Iter 2101 | sq_loss 0.00000184\n",
      "Iter 2201 | sq_loss 0.00000166\n",
      "Iter 2301 | sq_loss 0.00000153\n",
      "Iter 2401 | sq_loss 0.00000144\n",
      "Iter 2501 | sq_loss 0.00000137\n",
      "Iter 2601 | sq_loss 0.00000132\n",
      "Iter 2701 | sq_loss 0.00000129\n",
      "Iter 2801 | sq_loss 0.00000125\n",
      "Iter 2901 | sq_loss 0.00000123\n",
      "Iter 3001 | sq_loss 0.00000120\n",
      "Iter 3101 | sq_loss 0.00000117\n",
      "Iter 3201 | sq_loss 0.00000115\n",
      "Iter 3301 | sq_loss 0.00000112\n",
      "Iter 3401 | sq_loss 0.00000109\n",
      "Iter 3501 | sq_loss 0.00000107\n",
      "Iter 3601 | sq_loss 0.00000104\n",
      "Iter 3701 | sq_loss 0.00000101\n",
      "Bootstrap 11 --------------------------------------------------\n",
      "Iter 1 | loss 2.4639\n",
      "Iter 101 | loss 1.7377\n",
      "Iter 201 | loss 1.6682\n",
      "Iter 301 | loss 1.6440\n",
      "Iter 401 | loss 1.6342\n",
      "Iter 501 | loss 1.6299\n",
      "Iter 601 | loss 1.6278\n",
      "Iter 701 | loss 1.6267\n",
      "Iter 801 | loss 1.6260\n",
      "Iter 901 | loss 1.6255\n",
      "Iter 1001 | loss 1.6251\n",
      "Iter 1101 | loss 1.6247\n",
      "Iter 1201 | loss 1.6244\n",
      "Iter 1301 | loss 1.6240\n",
      "Iter 1401 | loss 1.6236\n",
      "Iter 1501 | loss 1.6233\n",
      "Iter 1601 | loss 1.6229\n",
      "Iter 1701 | loss 1.6224\n",
      "Iter 1801 | loss 1.6219\n",
      "Iter 1901 | loss 1.6212\n",
      "Iter 2001 | loss 1.6204\n",
      "Iter 2101 | loss 1.6191\n",
      "Iter 2201 | loss 1.6168\n",
      "Iter 2301 | loss 1.6116\n",
      "Iter 2401 | loss 1.5902\n",
      "Iter 2501 | loss 1.5532\n",
      "Iter 2601 | loss 1.5385\n",
      "Iter 2701 | loss 1.5036\n",
      "Iter 2801 | loss 1.4927\n",
      "Iter 2901 | loss 1.4918\n",
      "Iter 3001 | loss 1.4936\n",
      "Iter 3101 | loss 1.4932\n",
      "Iter 3201 | loss 1.4925\n",
      "Iter 3301 | loss 1.4920\n",
      "Iter 1 | sq_loss 0.00237541\n",
      "Iter 101 | sq_loss 0.00001639\n",
      "Iter 201 | sq_loss 0.00001513\n",
      "Iter 301 | sq_loss 0.00001362\n",
      "Iter 401 | sq_loss 0.00001198\n",
      "Iter 501 | sq_loss 0.00001034\n",
      "Iter 601 | sq_loss 0.00000878\n",
      "Iter 701 | sq_loss 0.00000734\n",
      "Iter 801 | sq_loss 0.00000609\n",
      "Iter 901 | sq_loss 0.00000502\n",
      "Iter 1001 | sq_loss 0.00000416\n",
      "Iter 1101 | sq_loss 0.00000347\n",
      "Iter 1201 | sq_loss 0.00000294\n",
      "Iter 1301 | sq_loss 0.00000256\n",
      "Iter 1401 | sq_loss 0.00000229\n",
      "Iter 1501 | sq_loss 0.00000210\n",
      "Iter 1601 | sq_loss 0.00000197\n",
      "Iter 1701 | sq_loss 0.00000188\n",
      "Iter 1801 | sq_loss 0.00000183\n",
      "Iter 1901 | sq_loss 0.00000179\n",
      "Iter 2001 | sq_loss 0.00000177\n",
      "Iter 2101 | sq_loss 0.00000175\n",
      "Iter 2201 | sq_loss 0.00000173\n",
      "Iter 2301 | sq_loss 0.00000172\n",
      "Iter 2401 | sq_loss 0.00000170\n",
      "Iter 2501 | sq_loss 0.00000169\n",
      "Iter 2601 | sq_loss 0.00000168\n",
      "Iter 2701 | sq_loss 0.00000166\n",
      "Iter 2801 | sq_loss 0.00000165\n",
      "Iter 2901 | sq_loss 0.00000163\n",
      "Iter 3001 | sq_loss 0.00000162\n",
      "Iter 3101 | sq_loss 0.00000160\n",
      "Iter 3201 | sq_loss 0.00000159\n",
      "Iter 3301 | sq_loss 0.00000157\n",
      "Iter 3401 | sq_loss 0.00000155\n",
      "Iter 3501 | sq_loss 0.00000154\n",
      "Iter 3601 | sq_loss 0.00000152\n",
      "Iter 3701 | sq_loss 0.00000150\n",
      "Iter 3801 | sq_loss 0.00000148\n",
      "Iter 3901 | sq_loss 0.00000146\n",
      "Iter 4001 | sq_loss 0.00000144\n",
      "Iter 4101 | sq_loss 0.00000143\n",
      "Iter 4201 | sq_loss 0.00000141\n",
      "Iter 4301 | sq_loss 0.00000139\n",
      "Iter 4401 | sq_loss 0.00000137\n",
      "Iter 4501 | sq_loss 0.00000135\n",
      "Iter 4601 | sq_loss 0.00000133\n",
      "Iter 4701 | sq_loss 0.00000131\n",
      "Iter 4801 | sq_loss 0.00000129\n",
      "Iter 4901 | sq_loss 0.00000127\n",
      "Iter 5001 | sq_loss 0.00000125\n",
      "Iter 5101 | sq_loss 0.00000123\n",
      "Iter 5201 | sq_loss 0.00000121\n",
      "Iter 5301 | sq_loss 0.00000119\n",
      "Iter 5401 | sq_loss 0.00000118\n",
      "Iter 5501 | sq_loss 0.00000116\n",
      "Iter 5601 | sq_loss 0.00000114\n",
      "Iter 5701 | sq_loss 0.00000112\n",
      "Iter 5801 | sq_loss 0.00000111\n",
      "Iter 5901 | sq_loss 0.00000109\n",
      "Iter 6001 | sq_loss 0.00000107\n",
      "Iter 6101 | sq_loss 0.00000106\n",
      "Iter 6201 | sq_loss 0.00000104\n",
      "Iter 6301 | sq_loss 0.00000103\n",
      "Iter 6401 | sq_loss 0.00000101\n",
      "Bootstrap 12 --------------------------------------------------\n",
      "Iter 1 | loss 1.6363\n",
      "Iter 101 | loss 1.4907\n",
      "Iter 201 | loss 1.4765\n",
      "Iter 301 | loss 1.4735\n",
      "Iter 401 | loss 1.4719\n",
      "Iter 501 | loss 1.4707\n",
      "Iter 601 | loss 1.4698\n",
      "Iter 701 | loss 1.4690\n",
      "Iter 801 | loss 1.4683\n",
      "Iter 901 | loss 1.4678\n",
      "Iter 1001 | loss 1.4673\n",
      "Iter 1101 | loss 1.4668\n",
      "Iter 1201 | loss 1.4664\n",
      "Iter 1301 | loss 1.4661\n",
      "Iter 1401 | loss 1.4657\n",
      "Iter 1501 | loss 1.4654\n",
      "Iter 1601 | loss 1.4652\n",
      "Iter 1701 | loss 1.4649\n",
      "Iter 1801 | loss 1.4647\n",
      "Iter 1901 | loss 1.4644\n",
      "Iter 2001 | loss 1.4642\n",
      "Iter 2101 | loss 1.4640\n",
      "Iter 2201 | loss 1.4639\n",
      "Iter 2301 | loss 1.4637\n",
      "Iter 2401 | loss 1.4636\n",
      "Iter 2501 | loss 1.4634\n",
      "Iter 1 | sq_loss 0.00000205\n",
      "Bootstrap 13 --------------------------------------------------\n",
      "Iter 1 | loss 1.6755\n",
      "Iter 101 | loss 1.5510\n",
      "Iter 201 | loss 1.5331\n",
      "Iter 301 | loss 1.5260\n",
      "Iter 401 | loss 1.5243\n",
      "Iter 501 | loss 1.5217\n",
      "Iter 601 | loss 1.5155\n",
      "Iter 701 | loss 1.5069\n",
      "Iter 801 | loss 1.5055\n",
      "Iter 1 | sq_loss 0.00481895\n",
      "Bootstrap 14 --------------------------------------------------\n",
      "Iter 1 | loss 1.9505\n",
      "Iter 101 | loss 1.6585\n",
      "Iter 201 | loss 1.5582\n",
      "Iter 301 | loss 1.5387\n",
      "Iter 401 | loss 1.5325\n",
      "Iter 501 | loss 1.5294\n",
      "Iter 601 | loss 1.5273\n",
      "Iter 701 | loss 1.5260\n",
      "Iter 801 | loss 1.5251\n",
      "Iter 901 | loss 1.5248\n",
      "Iter 1001 | loss 1.5242\n",
      "Iter 1101 | loss 1.5231\n",
      "Iter 1201 | loss 1.5243\n",
      "Iter 1301 | loss 1.5250\n",
      "Iter 1 | sq_loss 0.00665388\n",
      "Iter 101 | sq_loss 0.00004186\n",
      "Iter 201 | sq_loss 0.00003947\n",
      "Iter 301 | sq_loss 0.00003688\n",
      "Iter 401 | sq_loss 0.00003393\n",
      "Iter 501 | sq_loss 0.00003077\n",
      "Iter 601 | sq_loss 0.00002751\n",
      "Iter 701 | sq_loss 0.00002426\n",
      "Iter 801 | sq_loss 0.00002111\n",
      "Iter 901 | sq_loss 0.00001812\n",
      "Iter 1001 | sq_loss 0.00001535\n",
      "Iter 1101 | sq_loss 0.00001285\n",
      "Iter 1201 | sq_loss 0.00001064\n",
      "Iter 1301 | sq_loss 0.00000872\n",
      "Iter 1401 | sq_loss 0.00000711\n",
      "Iter 1501 | sq_loss 0.00000578\n",
      "Iter 1601 | sq_loss 0.00000470\n",
      "Iter 1701 | sq_loss 0.00000385\n",
      "Iter 1801 | sq_loss 0.00000320\n",
      "Iter 1901 | sq_loss 0.00000271\n",
      "Iter 2001 | sq_loss 0.00000234\n",
      "Iter 2101 | sq_loss 0.00000208\n",
      "Iter 2201 | sq_loss 0.00000190\n",
      "Iter 2301 | sq_loss 0.00000177\n",
      "Iter 2401 | sq_loss 0.00000169\n",
      "Iter 2501 | sq_loss 0.00000163\n",
      "Iter 2601 | sq_loss 0.00000159\n",
      "Iter 2701 | sq_loss 0.00000156\n",
      "Iter 2801 | sq_loss 0.00000154\n",
      "Iter 2901 | sq_loss 0.00000152\n",
      "Iter 3001 | sq_loss 0.00000150\n",
      "Iter 3101 | sq_loss 0.00000149\n",
      "Iter 3201 | sq_loss 0.00000148\n",
      "Iter 3301 | sq_loss 0.00000146\n",
      "Iter 3401 | sq_loss 0.00000145\n",
      "Iter 3501 | sq_loss 0.00000144\n",
      "Iter 3601 | sq_loss 0.00000143\n",
      "Iter 3701 | sq_loss 0.00000141\n",
      "Iter 3801 | sq_loss 0.00000140\n",
      "Iter 3901 | sq_loss 0.00000139\n",
      "Iter 4001 | sq_loss 0.00000137\n",
      "Iter 4101 | sq_loss 0.00000136\n",
      "Iter 4201 | sq_loss 0.00000134\n",
      "Iter 4301 | sq_loss 0.00000133\n",
      "Iter 4401 | sq_loss 0.00000132\n",
      "Iter 4501 | sq_loss 0.00000130\n",
      "Iter 4601 | sq_loss 0.00000129\n",
      "Iter 4701 | sq_loss 0.00000128\n",
      "Iter 4801 | sq_loss 0.00000126\n",
      "Iter 4901 | sq_loss 0.00000125\n",
      "Iter 5001 | sq_loss 0.00000124\n",
      "Iter 5101 | sq_loss 0.00000122\n",
      "Iter 5201 | sq_loss 0.00000121\n",
      "Iter 5301 | sq_loss 0.00000120\n",
      "Iter 5401 | sq_loss 0.00000119\n",
      "Iter 5501 | sq_loss 0.00000118\n",
      "Iter 5601 | sq_loss 0.00000117\n",
      "Iter 5701 | sq_loss 0.00000115\n",
      "Iter 5801 | sq_loss 0.00000114\n",
      "Iter 5901 | sq_loss 0.00000113\n",
      "Iter 6001 | sq_loss 0.00000112\n",
      "Iter 6101 | sq_loss 0.00000111\n",
      "Iter 6201 | sq_loss 0.00000110\n",
      "Iter 6301 | sq_loss 0.00000109\n",
      "Iter 6401 | sq_loss 0.00000108\n",
      "Iter 6501 | sq_loss 0.00000107\n",
      "Iter 6601 | sq_loss 0.00000106\n",
      "Iter 6701 | sq_loss 0.00000105\n",
      "Iter 6801 | sq_loss 0.00000104\n",
      "Iter 6901 | sq_loss 0.00000103\n",
      "Iter 7001 | sq_loss 0.00000101\n",
      "Iter 7101 | sq_loss 0.00000100\n",
      "Bootstrap 15 --------------------------------------------------\n",
      "Iter 1 | loss 1.6340\n",
      "Iter 101 | loss 1.4878\n",
      "Iter 201 | loss 1.4750\n",
      "Iter 301 | loss 1.4713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f27d350d6cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mXdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmlealpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrwalpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlebeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmleeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlegamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmlealphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlealpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdrwalphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrwalpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6d0b8d9e5ec6>\u001b[0m in \u001b[0;36mMLE\u001b[0;34m(X, Z, D, Y, estimator, dr)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0moptloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moptloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6d0b8d9e5ec6>\u001b[0m in \u001b[0;36mnll\u001b[0;34m(X, Z, D, Y, alpha, beta, eta, gamma, estimator)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MLATE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc5\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc5\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc5\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mc5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mp011\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = torch.load('401k.pt')\n",
    "data[:,1] /= 100\n",
    "data[:,4] /= 10\n",
    "data[:,9] /= 10000\n",
    "Z = data[:,0]\n",
    "X = torch.cat((torch.ones((data.shape[0],1)), data[:, [1,2,4,5,9]]), dim=-1)\n",
    "D = data[:,7]\n",
    "Y = data[:,8]\n",
    "\n",
    "N, p = X.shape\n",
    "NR = 500\n",
    "torch.manual_seed(6971)\n",
    "mlealphas = torch.zeros(size=(NR, p))\n",
    "drwalphas = torch.zeros(size=(NR, p))\n",
    "minimums, optlosses = [], []\n",
    "for i in range(NR):\n",
    "    print('Bootstrap {}'.format(i+1), '-'*50)\n",
    "    idxes = torch.multinomial(torch.ones(N), N, replacement=True)\n",
    "    Xdata = X[idxes].clone()\n",
    "    mlealpha, drwalpha, mlebeta, mleeta, mlegamma = MLE(Xdata,Z,D,Y)\n",
    "    mlealphas[i] = mlealpha\n",
    "    drwalphas[i] = drwalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0550,  1.6181, -0.0665,  0.0621, -0.0297, -1.3122],\n",
       "        [-0.0221, -0.0122,  0.0806,  0.0280,  0.0153,  0.0976],\n",
       "        [ 0.2240,  0.0399, -0.1309,  0.0756, -0.0317, -0.1542],\n",
       "        [ 0.3013, -0.7200, -0.0963,  0.1174,  0.0084,  0.3663],\n",
       "        [ 0.2606,  0.4524, -0.4480,  0.0324,  0.1154, -0.3442],\n",
       "        [ 0.3778,  0.1074,  0.0386, -0.0704,  0.0302,  0.0537],\n",
       "        [ 0.5544,  0.5994, -0.2681, -0.0059,  0.0099, -0.3899],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlealphas[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0459, 0.0135, 0.0143, 0.1103, 0.0092, 0.0888])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drwalphas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
