{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(X,Z,D,Y,alpha,beta,eta,gamma,estimator='LATE'):\n",
    "    # print(X.type(), beta.type())\n",
    "    phi = torch.sigmoid(X@beta)\n",
    "    c1, c3 = phi[:,0], phi[:,1]\n",
    "    if estimator == 'MLATE':\n",
    "        c0 = torch.exp(X @ alpha)\n",
    "    c5 = torch.exp(X@eta)\n",
    "    # c5 = c5 * (torch.abs(c5-1) > 1e-10) + (torch.abs(c5-1) <= 1e-10) * (c5-1 >= 0.0) * (1+1e-10) + (torch.abs(c5-1) <= 1e-10) * (c5-1< 0.0) * (1-1e-10)\n",
    "    # c5 = 1 + c5 - c5.clamp(1-1e-5,1+1e-5)\n",
    "    c0 = c0.clamp(1e-10)\n",
    "    if estimator == 'MLATE':\n",
    "        f0 = torch.where(torch.abs(c5-1)>1e-10, (-(c0+1)*c5+torch.sqrt(c5**2*(c0-1)**2 + 4*c0*c5)) / (2*c0*(1-c5)), -(-c0-1+(2*(c0-1)**2+4*c0)/(c0+1)/2)/(2*c0))\n",
    "        f1 = f0 * c0\n",
    "    p011 = (1-c1)*c3\n",
    "    p001 = (1-c1)*(1-c3)\n",
    "    p110 = 0\n",
    "    p100 = 0\n",
    "    p111 = f1*c1 + p110\n",
    "    p010 = f0*c1 + p011\n",
    "    p101 = 1-p001-p011-p111\n",
    "    p000 = 1-p010-p100-p110\n",
    "\n",
    "    d = torch.sigmoid(X@gamma)\n",
    "    l = D*Y*Z*p111*d + (1-D)*Y*Z*p011*d + D*(1-Y)*Z*p101*d + (1-D)*(1-Y)*Z*p001*d + (1-D)*Y*(1-Z)*p010*(1-d) + (1-D)*(1-Y)*(1-Z)*p000*(1-d)\n",
    "    # if torch.mean(torch.log(l.clamp(1e-10, 1))) :\n",
    "    #     print(l)\n",
    "    #     sys.exit(0)\n",
    "    return torch.mean(torch.log(l.clamp(1e-10)))\n",
    "\n",
    "def square_loss(X, Z, D, Y, alpha, beta, gamma, eta, estimator, strategy='identity'):\n",
    "    d = torch.sigmoid(X@gamma)\n",
    "    phi = torch.sigmoid(X@beta)\n",
    "    phi1, phi3 = phi[:,0], phi[:,1]\n",
    "    OP = torch.exp(X@eta)\n",
    "    f = (d**Z) * ((1-d)**(1-Z))\n",
    "    if estimator == 'MLATE':\n",
    "        theta = torch.exp(X@alpha)\n",
    "        H = Y * theta**(-D)\n",
    "        f0 = torch.where(torch.abs(OP-1)>1e-10, (-(theta+1)*OP+torch.sqrt(OP**2*(theta-1)**2+4*theta*OP)) / (2*theta*(1-OP)), -(-theta-1+(2*(theta-1)**2+4*theta)/(theta+1)/2)/(2*theta))\n",
    "        f1 = f0 * theta\n",
    "        E = f0*phi1 +(1-phi1)*phi3\n",
    "    \n",
    "    if strategy == 'identity':\n",
    "        return torch.sum((torch.sum(X*((2*Z-1)*(H-E)/f).unsqueeze(1), dim=0))**2)\n",
    "    elif strategy == 'optimal':\n",
    "        p011 = (1-phi1)*phi3\n",
    "        p001 = (1-phi1)*(1-phi3)\n",
    "        p110 = 0\n",
    "        p100 = 0\n",
    "        p111 = f1*phi1 + p110\n",
    "        p010 = f0*phi1 + p011\n",
    "        p101 = 1-p001-p011-p111\n",
    "        p000 = 1-p010-p100-p110\n",
    "        if estimator == 'MLATE':\n",
    "            EH2_1 = p111/theta**2+p101\n",
    "            EH2_0 = p110/theta**2+p100\n",
    "            EH_1 = p111/theta+p101\n",
    "            EH_0 = p110/theta+p100\n",
    "            EZX = (EH2_1-EH_1**2) / d + (EH2_0-EH_0**2)/(1-d)\n",
    "            w = -X * (1 / theta * f1 * phi1 / EZX).unsqueeze(1)\n",
    "            return torch.mean((torch.mean(w*((2*Z-1)*(H-E)/f).unsqueeze(1), dim=0))**2)\n",
    "\n",
    "def MLE(X, Z, D ,Y, estimator='MLATE', dr=True):\n",
    "    N, p = X.shape\n",
    "    alpha = nn.Parameter(torch.rand(size=(p,))*0.2-0.1)\n",
    "    beta = nn.Parameter(torch.rand(size=(p,2))*0.2-0.1) ## only phi1 and phi3\n",
    "    eta = nn.Parameter(torch.rand(size=(p,))*2-1)\n",
    "    gamma = nn.Parameter(torch.rand(size=(p,))*0.2-0.1)\n",
    "    opt = torch.optim.Adam(params=(alpha, beta, eta, gamma), lr=1e-3, weight_decay=0)\n",
    "    optloss = float('inf')\n",
    "    for i in range(50000):\n",
    "        opt.zero_grad()\n",
    "        loss = -nll(X,Z,D,Y,alpha,beta,eta,gamma, estimator)\n",
    "        if loss.item() < optloss:\n",
    "            if abs(loss.item() - optloss) < 1e-7:\n",
    "                break\n",
    "            optloss = loss.item()\n",
    "            mlealpha = alpha.detach().clone()\n",
    "            mlebeta = beta.detach().clone()\n",
    "            mleeta = eta.detach().clone()\n",
    "            mlegamma = gamma.detach().clone()\n",
    "        if i % 100 ==0: \n",
    "            print('Iter {} | loss {:.04f}'.format(i+1, loss.item()))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if torch.sum(loss.isnan()) > 0:\n",
    "            break\n",
    "    if not dr:\n",
    "        return mlealpha, mlebeta, mleeta, mlegamma\n",
    "        \n",
    "    alpha = nn.Parameter(mlealpha.clone(),requires_grad=True)\n",
    "    # alpha = nn.Parameter(torch.rand(size=(2,))*2-1)\n",
    "    opt = torch.optim.Adam(params=(alpha,), lr=1e-3, weight_decay=0)\n",
    "    sqoptloss = float('inf')\n",
    "    for i in range(50000):\n",
    "        opt.zero_grad()\n",
    "        sq_loss = square_loss(X, Z, D, Y, alpha, mlebeta, mlegamma, mleeta, estimator, strategy='optimal')\n",
    "        if i % 100 ==0:\n",
    "            print('Iter {} | sq_loss {:.08f}'.format(i+1, sq_loss.item()))\n",
    "        if sq_loss.item() < sqoptloss:\n",
    "            sqoptloss = sq_loss.item()\n",
    "            drwalpha = alpha.detach().clone()\n",
    "            if abs(sqoptloss) < 1e-6:\n",
    "                break\n",
    "        sq_loss.backward()\n",
    "        opt.step()\n",
    "        if torch.sum(sq_loss.isnan()) > 0:\n",
    "            break\n",
    "    return mlealpha, drwalpha, mlebeta, mleeta, mlegamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 1 --------------------------------------------------\n",
      "Iter 1 | loss 1.5797\n",
      "Iter 101 | loss 1.5060\n",
      "Iter 201 | loss 1.4943\n",
      "Iter 301 | loss 1.4887\n",
      "Iter 401 | loss 1.4853\n",
      "Iter 501 | loss 1.4837\n",
      "Iter 601 | loss 1.4807\n",
      "Iter 701 | loss 1.4795\n",
      "Iter 801 | loss 1.4786\n",
      "Iter 901 | loss 1.4780\n",
      "Iter 1001 | loss 1.4775\n",
      "Iter 1101 | loss 1.4770\n",
      "Iter 1201 | loss 1.4767\n",
      "Iter 1301 | loss 1.4763\n",
      "Iter 1401 | loss 1.4760\n",
      "Iter 1501 | loss 1.4758\n",
      "Iter 1601 | loss 1.4755\n",
      "Iter 1701 | loss 1.4753\n",
      "Iter 1801 | loss 1.4750\n",
      "Iter 1901 | loss 1.4748\n",
      "Iter 2001 | loss 1.4746\n",
      "Iter 2101 | loss 1.4743\n",
      "Iter 2201 | loss 1.4741\n",
      "Iter 2301 | loss 1.4739\n",
      "Iter 2401 | loss 1.4736\n",
      "Iter 2501 | loss 1.4734\n",
      "Iter 2601 | loss 1.4732\n",
      "Iter 2701 | loss 1.4729\n",
      "Iter 2801 | loss 1.4727\n",
      "Iter 2901 | loss 1.4725\n",
      "Iter 3001 | loss 1.4722\n",
      "Iter 3101 | loss 1.4720\n",
      "Iter 3201 | loss 1.4718\n",
      "Iter 3301 | loss 1.4715\n",
      "Iter 3401 | loss 1.4713\n",
      "Iter 3501 | loss 1.4711\n",
      "Iter 3601 | loss 1.4709\n",
      "Iter 3701 | loss 1.4706\n",
      "Iter 3801 | loss 1.4704\n",
      "Iter 3901 | loss 1.4702\n",
      "Iter 4001 | loss 1.4700\n",
      "Iter 4101 | loss 1.4698\n",
      "Iter 4201 | loss 1.4695\n",
      "Iter 4301 | loss 1.4693\n",
      "Iter 4401 | loss 1.4691\n",
      "Iter 4501 | loss 1.4689\n",
      "Iter 4601 | loss 1.4687\n",
      "Iter 4701 | loss 1.4685\n",
      "Iter 4801 | loss 1.4683\n",
      "Iter 4901 | loss 1.4681\n",
      "Iter 5001 | loss 1.4679\n",
      "Iter 5101 | loss 1.4678\n",
      "Iter 5201 | loss 1.4676\n",
      "Iter 5301 | loss 1.4674\n",
      "Iter 5401 | loss 1.4672\n",
      "Iter 5501 | loss 1.4670\n",
      "Iter 5601 | loss 1.4669\n",
      "Iter 5701 | loss 1.4667\n",
      "Iter 5801 | loss 1.4665\n",
      "Iter 5901 | loss 1.4664\n",
      "Iter 6001 | loss 1.4662\n",
      "Iter 6101 | loss 1.4661\n",
      "Iter 6201 | loss 1.4659\n",
      "Iter 6301 | loss 1.4658\n",
      "Iter 6401 | loss 1.4656\n",
      "Iter 6501 | loss 1.4655\n",
      "Iter 6601 | loss 1.4654\n",
      "Iter 6701 | loss 1.4652\n",
      "Iter 6801 | loss 1.4651\n",
      "Iter 6901 | loss 1.4650\n",
      "Iter 7001 | loss 1.4649\n",
      "Iter 7101 | loss 1.4647\n",
      "Iter 7201 | loss 1.4646\n",
      "Iter 7301 | loss 1.4645\n",
      "Iter 7401 | loss 1.4645\n",
      "Iter 7501 | loss 1.4644\n",
      "Iter 7601 | loss 1.4643\n",
      "Iter 7701 | loss 1.4642\n",
      "Iter 7801 | loss 1.4641\n",
      "Iter 7901 | loss 1.4641\n",
      "Iter 8001 | loss 1.4640\n",
      "Iter 8101 | loss 1.4640\n",
      "Iter 8201 | loss 1.4639\n",
      "Iter 8301 | loss 1.4639\n",
      "Iter 8401 | loss 1.4638\n",
      "Iter 8501 | loss 1.4638\n",
      "Iter 8601 | loss 1.4638\n",
      "Iter 8701 | loss 1.4637\n",
      "Iter 8801 | loss 1.4637\n",
      "Iter 8901 | loss 1.4636\n",
      "Iter 9001 | loss 1.4636\n",
      "Iter 9101 | loss 1.4636\n",
      "Iter 9201 | loss 1.4635\n",
      "Iter 9301 | loss 1.4635\n",
      "Iter 9401 | loss 1.4634\n",
      "Iter 9501 | loss 1.4633\n",
      "Iter 9601 | loss 1.4632\n",
      "Iter 9701 | loss 1.4631\n",
      "Iter 9801 | loss 1.4631\n",
      "Iter 9901 | loss 1.4630\n",
      "Iter 10001 | loss 1.4629\n",
      "Iter 10101 | loss 1.4629\n",
      "Iter 10201 | loss 1.4629\n",
      "Iter 10301 | loss 1.4628\n",
      "Iter 10401 | loss 1.4628\n",
      "Iter 10501 | loss 1.4627\n",
      "Iter 10601 | loss 1.4627\n",
      "Iter 10701 | loss 1.4626\n",
      "Iter 10801 | loss 1.4626\n",
      "Iter 10901 | loss 1.4626\n",
      "Iter 11001 | loss 1.4625\n",
      "Iter 11101 | loss 1.4625\n",
      "Iter 11201 | loss 1.4625\n",
      "Iter 11301 | loss 1.4624\n",
      "Iter 11401 | loss 1.4624\n",
      "Iter 11501 | loss 1.4623\n",
      "Iter 11601 | loss 1.4623\n",
      "Iter 11701 | loss 1.4623\n",
      "Iter 11801 | loss 1.4622\n",
      "Iter 11901 | loss 1.4622\n",
      "Iter 12001 | loss 1.4622\n",
      "Iter 12101 | loss 1.4622\n",
      "Iter 12201 | loss 1.4621\n",
      "Iter 12301 | loss 1.4621\n",
      "Iter 12401 | loss 1.4621\n",
      "Iter 12501 | loss 1.4620\n",
      "Iter 12601 | loss 1.4620\n",
      "Iter 12701 | loss 1.4620\n",
      "Iter 12801 | loss 1.4620\n",
      "Iter 12901 | loss 1.4620\n",
      "Iter 13001 | loss 1.4620\n",
      "Iter 13101 | loss 1.4619\n",
      "Iter 13201 | loss 1.4619\n",
      "Iter 13301 | loss 1.4619\n",
      "Iter 13401 | loss 1.4619\n",
      "Iter 13501 | loss 1.4619\n",
      "Iter 13601 | loss 1.4619\n",
      "Iter 13701 | loss 1.4619\n",
      "Iter 13801 | loss 1.4619\n",
      "Iter 13901 | loss 1.4619\n",
      "Iter 14001 | loss 1.4619\n",
      "Iter 14101 | loss 1.4618\n",
      "Iter 14201 | loss 1.4618\n",
      "Iter 14301 | loss 1.4618\n",
      "Iter 14401 | loss 1.4618\n",
      "Iter 14501 | loss 1.4618\n",
      "Iter 14601 | loss 1.4618\n",
      "Iter 14701 | loss 1.4618\n",
      "Iter 14801 | loss 1.4618\n",
      "Iter 14901 | loss 1.4618\n",
      "Iter 15001 | loss 1.4618\n",
      "Iter 15101 | loss 1.4618\n",
      "Iter 15201 | loss 1.4618\n",
      "Iter 15301 | loss 1.4618\n",
      "Iter 15401 | loss 1.4618\n",
      "Iter 15501 | loss 1.4618\n",
      "Iter 15601 | loss 1.4618\n",
      "Iter 15701 | loss 1.4618\n",
      "Iter 15801 | loss 1.4618\n",
      "Iter 15901 | loss 1.4618\n",
      "Iter 16001 | loss 1.4618\n",
      "Iter 16101 | loss 1.4618\n",
      "Iter 16201 | loss 1.4618\n",
      "Iter 16301 | loss 1.4618\n",
      "Iter 16401 | loss 1.4618\n",
      "Iter 16501 | loss 1.4618\n",
      "Iter 16601 | loss 1.4618\n",
      "Iter 16701 | loss 1.4618\n",
      "Iter 16801 | loss 1.4618\n",
      "Iter 16901 | loss 1.4618\n",
      "Iter 17001 | loss 1.4618\n",
      "Iter 17101 | loss 1.4618\n",
      "Iter 17201 | loss 1.4618\n",
      "Iter 17301 | loss 1.4618\n",
      "Iter 17401 | loss 1.4618\n",
      "Iter 17501 | loss 1.4618\n",
      "Iter 17601 | loss 1.4618\n",
      "Iter 17701 | loss 1.4618\n",
      "Iter 17801 | loss 1.4618\n",
      "Iter 17901 | loss 1.4618\n",
      "Iter 18001 | loss 1.4618\n",
      "Iter 18101 | loss 1.4618\n",
      "Iter 18201 | loss 1.4618\n",
      "Iter 18301 | loss 1.4618\n",
      "Iter 18401 | loss 1.4618\n",
      "Iter 18501 | loss 1.4618\n",
      "Iter 18601 | loss 1.4618\n",
      "Iter 18701 | loss 1.4618\n",
      "Iter 18801 | loss 1.4618\n",
      "Iter 18901 | loss 1.4618\n",
      "Iter 19001 | loss 1.4618\n",
      "Iter 19101 | loss 1.4618\n",
      "Iter 19201 | loss 1.4618\n",
      "Iter 19301 | loss 1.4618\n",
      "Iter 19401 | loss 1.4618\n",
      "Iter 19501 | loss 1.4618\n",
      "Iter 19601 | loss 1.4618\n",
      "Iter 19701 | loss 1.4618\n",
      "Iter 19801 | loss 1.4618\n",
      "Iter 19901 | loss 1.4618\n",
      "Iter 20001 | loss 1.4618\n",
      "Iter 20101 | loss 1.4618\n",
      "Iter 20201 | loss 1.4618\n",
      "Iter 20301 | loss 1.4618\n",
      "Iter 20401 | loss 1.4618\n",
      "Iter 20501 | loss 1.4618\n",
      "Iter 20601 | loss 1.4618\n",
      "Iter 20701 | loss 1.4618\n",
      "Iter 20801 | loss 1.4618\n",
      "Iter 20901 | loss 1.4618\n",
      "Iter 21001 | loss 1.4618\n",
      "Iter 21101 | loss 1.4618\n",
      "Iter 21201 | loss 1.4618\n",
      "Iter 21301 | loss 1.4618\n",
      "Iter 21401 | loss 1.4618\n",
      "Iter 21501 | loss 1.4618\n",
      "Iter 21601 | loss 1.4618\n",
      "Iter 21701 | loss 1.4618\n",
      "Iter 21801 | loss 1.4618\n",
      "Iter 21901 | loss 1.4618\n",
      "Iter 22001 | loss 1.4618\n",
      "Iter 22101 | loss 1.4618\n",
      "Iter 22201 | loss 1.4618\n",
      "Iter 22301 | loss 1.4618\n",
      "Iter 22401 | loss 1.4618\n",
      "Iter 22501 | loss 1.4618\n",
      "Iter 22601 | loss 1.4618\n",
      "Iter 22701 | loss 1.4618\n",
      "Iter 22801 | loss 1.4618\n",
      "Iter 22901 | loss 1.4618\n",
      "Iter 23001 | loss 1.4618\n",
      "Iter 23101 | loss 1.4618\n",
      "Iter 23201 | loss 1.4618\n",
      "Iter 23301 | loss 1.4618\n",
      "Iter 23401 | loss 1.4618\n",
      "Iter 23501 | loss 1.4618\n",
      "Iter 23601 | loss 1.4618\n",
      "Iter 23701 | loss 1.4618\n",
      "Iter 23801 | loss 1.4618\n",
      "Iter 23901 | loss 1.4618\n",
      "Iter 24001 | loss 1.4618\n",
      "Iter 24101 | loss 1.4618\n",
      "Iter 24201 | loss 1.4618\n",
      "Iter 24301 | loss 1.4618\n",
      "Iter 24401 | loss 1.4618\n",
      "Iter 24501 | loss 1.4618\n",
      "Iter 24601 | loss 1.4618\n",
      "Iter 24701 | loss 1.4618\n",
      "Iter 24801 | loss 1.4618\n",
      "Iter 24901 | loss 1.4618\n",
      "Iter 25001 | loss 1.4618\n",
      "Iter 25101 | loss 1.4618\n",
      "Iter 25201 | loss 1.4618\n",
      "Iter 25301 | loss 1.4618\n",
      "Iter 25401 | loss 1.4618\n",
      "Iter 25501 | loss 1.4618\n",
      "Iter 25601 | loss 1.4618\n",
      "Iter 25701 | loss 1.4618\n",
      "Iter 25801 | loss 1.4618\n",
      "Iter 25901 | loss 1.4618\n",
      "Iter 26001 | loss 1.4618\n",
      "Iter 26101 | loss 1.4618\n",
      "Iter 26201 | loss 1.4618\n",
      "Iter 26301 | loss 1.4618\n",
      "Iter 26401 | loss 1.4618\n",
      "Iter 26501 | loss 1.4618\n",
      "Iter 26601 | loss 1.4618\n",
      "Iter 26701 | loss 1.4618\n",
      "Iter 26801 | loss 1.4618\n",
      "Iter 26901 | loss 1.4618\n",
      "Iter 27001 | loss 1.4618\n",
      "Iter 27101 | loss 1.4618\n",
      "Iter 27201 | loss 1.4618\n",
      "Iter 27301 | loss 1.4618\n",
      "Iter 27401 | loss 1.4618\n",
      "Iter 27501 | loss 1.4618\n",
      "Iter 27601 | loss 1.4618\n",
      "Iter 27701 | loss 1.4618\n",
      "Iter 27801 | loss 1.4618\n",
      "Iter 27901 | loss 1.4618\n",
      "Iter 28001 | loss 1.4618\n",
      "Iter 28101 | loss 1.4618\n",
      "Iter 28201 | loss 1.4618\n",
      "Iter 28301 | loss 1.4618\n",
      "Iter 28401 | loss 1.4618\n",
      "Iter 28501 | loss 1.4618\n",
      "Iter 28601 | loss 1.4618\n",
      "Iter 28701 | loss 1.4618\n",
      "Iter 28801 | loss 1.4618\n",
      "Iter 28901 | loss 1.4618\n",
      "Iter 29001 | loss 1.4618\n",
      "Iter 29101 | loss 1.4618\n",
      "Iter 29201 | loss 1.4618\n",
      "Iter 29301 | loss 1.4618\n",
      "Iter 29401 | loss 1.4618\n",
      "Iter 29501 | loss 1.4618\n",
      "Iter 29601 | loss 1.4618\n",
      "Iter 29701 | loss 1.4618\n",
      "Iter 29801 | loss 1.4618\n",
      "Iter 29901 | loss 1.4618\n",
      "Iter 30001 | loss 1.4618\n",
      "Iter 30101 | loss 1.4618\n",
      "Iter 30201 | loss 1.4618\n",
      "Iter 30301 | loss 1.4618\n",
      "Iter 30401 | loss 1.4618\n",
      "Iter 30501 | loss 1.4618\n",
      "Iter 30601 | loss 1.4618\n",
      "Iter 30701 | loss 1.4618\n",
      "Iter 30801 | loss 1.4618\n",
      "Iter 30901 | loss 1.4618\n",
      "Iter 31001 | loss 1.4618\n",
      "Iter 31101 | loss 1.4618\n",
      "Iter 31201 | loss 1.4618\n",
      "Iter 31301 | loss 1.4618\n",
      "Iter 31401 | loss 1.4618\n",
      "Iter 31501 | loss 1.4618\n",
      "Iter 31601 | loss 1.4618\n",
      "Iter 31701 | loss 1.4618\n",
      "Iter 31801 | loss 1.4618\n",
      "Iter 31901 | loss 1.4618\n",
      "Iter 32001 | loss 1.4618\n",
      "Iter 32101 | loss 1.4618\n",
      "Iter 32201 | loss 1.4618\n",
      "Iter 32301 | loss 1.4618\n",
      "Iter 32401 | loss 1.4618\n",
      "Iter 32501 | loss 1.4618\n",
      "Iter 32601 | loss 1.4618\n",
      "Iter 32701 | loss 1.4618\n",
      "Iter 32801 | loss 1.4618\n",
      "Iter 32901 | loss 1.4618\n",
      "Iter 33001 | loss 1.4618\n",
      "Iter 33101 | loss 1.4618\n",
      "Iter 33201 | loss 1.4618\n",
      "Iter 33301 | loss 1.4618\n",
      "Iter 33401 | loss 1.4618\n",
      "Iter 33501 | loss 1.4618\n",
      "Iter 33601 | loss 1.4618\n",
      "Iter 33701 | loss 1.4618\n",
      "Iter 33801 | loss 1.4618\n",
      "Iter 33901 | loss 1.4618\n",
      "Iter 34001 | loss 1.4618\n",
      "Iter 34101 | loss 1.4618\n",
      "Iter 34201 | loss 1.4618\n",
      "Iter 34301 | loss 1.4618\n",
      "Iter 34401 | loss 1.4618\n",
      "Iter 34501 | loss 1.4618\n",
      "Iter 34601 | loss 1.4618\n",
      "Iter 34701 | loss 1.4618\n",
      "Iter 34801 | loss 1.4618\n",
      "Iter 34901 | loss 1.4618\n",
      "Iter 35001 | loss 1.4618\n",
      "Iter 35101 | loss 1.4618\n",
      "Iter 35201 | loss 1.4618\n",
      "Iter 35301 | loss 1.4618\n",
      "Iter 35401 | loss 1.4618\n",
      "Iter 35501 | loss 1.4618\n",
      "Iter 35601 | loss 1.4618\n",
      "Iter 35701 | loss 1.4618\n",
      "Iter 35801 | loss 1.4618\n",
      "Iter 35901 | loss 1.4618\n",
      "Iter 36001 | loss 1.4618\n",
      "Iter 36101 | loss 1.4618\n",
      "Iter 36201 | loss 1.4618\n",
      "Iter 36301 | loss 1.4618\n",
      "Iter 36401 | loss 1.4618\n",
      "Iter 36501 | loss 1.4618\n",
      "Iter 36601 | loss 1.4618\n",
      "Iter 36701 | loss 1.4618\n",
      "Iter 36801 | loss 1.4618\n",
      "Iter 36901 | loss 1.4618\n",
      "Iter 37001 | loss 1.4618\n",
      "Iter 37101 | loss 1.4618\n",
      "Iter 37201 | loss 1.4618\n",
      "Iter 37301 | loss 1.4618\n",
      "Iter 37401 | loss 1.4618\n",
      "Iter 37501 | loss 1.4618\n",
      "Iter 37601 | loss 1.4618\n",
      "Iter 37701 | loss 1.4618\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f27d350d6cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mXdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmlealpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrwalpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlebeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmleeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlegamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmlealphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlealpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdrwalphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrwalpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cbb6a89f5e3b>\u001b[0m in \u001b[0;36mMLE\u001b[0;34m(X, Z, D, Y, estimator, dr)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter {} | loss {:.04f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = torch.load('401k.pt')\n",
    "data[:,1] /= 100\n",
    "data[:,4] /= 10\n",
    "data[:,9] /= 10000\n",
    "Z = data[:,0]\n",
    "X = torch.cat((torch.ones((data.shape[0],1)), data[:, [1,2,4,5,9]]), dim=-1)\n",
    "D = data[:,7]\n",
    "Y = data[:,8]\n",
    "\n",
    "N, p = X.shape\n",
    "NR = 500\n",
    "torch.manual_seed(6971)\n",
    "mlealphas = torch.zeros(size=(NR, p))\n",
    "drwalphas = torch.zeros(size=(NR, p))\n",
    "minimums, optlosses = [], []\n",
    "for i in range(NR):\n",
    "    print('Bootstrap {}'.format(i+1), '-'*50)\n",
    "    idxes = torch.multinomial(torch.ones(N), N, replacement=True)\n",
    "    Xdata = X[idxes].clone()\n",
    "    mlealpha, drwalpha, mlebeta, mleeta, mlegamma = MLE(Xdata,Z,D,Y)\n",
    "    mlealphas[i] = mlealpha\n",
    "    drwalphas[i] = drwalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0550,  1.6181, -0.0665,  0.0621, -0.0297, -1.3122],\n",
       "        [-0.0221, -0.0122,  0.0806,  0.0280,  0.0153,  0.0976],\n",
       "        [ 0.2240,  0.0399, -0.1309,  0.0756, -0.0317, -0.1542],\n",
       "        [ 0.3013, -0.7200, -0.0963,  0.1174,  0.0084,  0.3663],\n",
       "        [ 0.2606,  0.4524, -0.4480,  0.0324,  0.1154, -0.3442],\n",
       "        [ 0.3778,  0.1074,  0.0386, -0.0704,  0.0302,  0.0537],\n",
       "        [ 0.5544,  0.5994, -0.2681, -0.0059,  0.0099, -0.3899],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlealphas[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0459, 0.0135, 0.0143, 0.1103, 0.0092, 0.0888])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drwalphas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
